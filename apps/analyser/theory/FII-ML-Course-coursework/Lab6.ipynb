{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Here is a step by step explanation of the algorithm: https://youtu.be/O2L2Uv9pdDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy\n",
    "\n",
    "Just as for the Decision Tree algorithm, accuracy in this case is defined as the proportion of correctly classified instances.\n",
    "For _training accuracy_ we can use the dedicated score function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data as usual\n",
    "import pandas as pd\n",
    "features = ['study', 'free', 'money']\n",
    "target = 'is_spam'\n",
    "messages = pd.DataFrame(\n",
    "  [(1, 0, 0, 0),\n",
    "  (0, 0, 1, 0),\n",
    "  (1, 0, 0, 0),\n",
    "  (1, 1, 0, 0)] +\n",
    "  [(0, 1, 0, 1)] * 4 +\n",
    "  [(0, 1, 1, 1)] * 4,\n",
    "columns=features+[target])\n",
    "messages\n",
    "\n",
    "# Fit a Naive Bayes classifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "X = messages[features]\n",
    "y = messages[target]\n",
    "cl = BernoulliNB().fit(X, y)\n",
    "\n",
    "# Measure accuracy on the training set\n",
    "cl.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is the same as predicting the training set and then counting how many\n",
    "predictions actually match the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = cl.predict(X)\n",
    "(y_hat == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, _cross-validation leave one out_ (_CVLOO_) works in the same way as for Decision Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVLOO scores: [1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Mean CVLOO score:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from statistics import mean\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(cl, X, y, cv=loo)\n",
    "print(\"CVLOO scores:\", scores)\n",
    "print(\"Mean CVLOO score: \", mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated features\n",
    "\n",
    "The naive assumption in the naive Bayes algorithm is that the features are independent. We can see what happens when this assumption is not true by adding a new feature to our dataset that is highly correlated with an existing one.\n",
    "\n",
    "For example, if we also consider the word ”school”, which appears roughly in the same message as the word ”study” and we want to predict \"study money\" we end up with the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study</th>\n",
       "      <th>free</th>\n",
       "      <th>money</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    study  free  money  school\n",
       "0       1     0      0       1\n",
       "1       0     0      1       0\n",
       "2       1     0      0       1\n",
       "3       1     1      0       1\n",
       "4       0     1      0       0\n",
       "5       0     1      0       0\n",
       "6       0     1      0       0\n",
       "7       0     1      0       0\n",
       "8       0     1      1       0\n",
       "9       0     1      1       0\n",
       "10      0     1      1       0\n",
       "11      0     1      1       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_messages = pd.DataFrame(\n",
    "  [(1, 0, 1)],\n",
    "columns = features)\n",
    "\n",
    "new_messages_dep = new_messages.copy()\n",
    "new_messages_dep['school'] = new_messages['study']\n",
    "X_dep = X.copy()\n",
    "X_dep['school'] = X['study']\n",
    "X_dep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the prediction when \"school\" is used and when \"school\" is not used as a training word. When the dependent variable is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98997649, 0.01002351]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlated variable is present\n",
    "cl = BernoulliNB().fit(X_dep, y)\n",
    "cl.predict_proba(new_messages_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when it is not present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93676815, 0.06323185]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlated variable not present\n",
    "cl = BernoulliNB().fit(X, y)\n",
    "cl.predict_proba(new_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of being spam decreased to 0.01 from 0.063 since the presence of the word ”school” also contributes toward this new probability. This can lead to incorrect predictions since ”school” brings no new information as long as ”study” is already considered, so should not have changed the prediction.\n",
    "\n",
    "We can compare this with the behaviour of ID3. When the dependent variable is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlated variable is present\n",
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy').fit(X_dep,y)\n",
    "dt.predict_proba(new_messages_dep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when it is not present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlated variable not present\n",
    "dt = tree.DecisionTreeClassifier(criterion='entropy').fit(X,y)\n",
    "dt.predict_proba(new_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, ID3 is not influenced by the cloned variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Laplace estimator\n",
    "\n",
    "Apart from the naivety assumption, there is another phenomenon affecting the predictions of Naive Bayes, when rare words are encountered. For instance, we saw that the message \"study money\" is predicted as non-spam with 93% probability, which seems reasonable. But what if we need to predict the message \"study money university\", and the word \"university\" has never been seen in a spam message before, only in regular messages?\n",
    "\n",
    "If we look at the naive Bayes formula $ v_{MAP} = \\mbox{argmax}_{v_j \\in V} \\prod_i P(a_i|v_j)P(v_j)$, we can see that all the terms are multiplied, while the term $P(\\mbox{university}|\\mbox{spam})$ is equal to 0, which makes the entire probability of \"spam\" become exactly 0.\n",
    "\n",
    "This is unrealistic since there are many rare or misspelled words which can turn a prediction to 0 instantly, completely cancelling the effect of all the other words.\n",
    "\n",
    "The Python implementation however, doesn't seem to give a 0% chance of spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98015192, 0.01984808]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data as usual\n",
    "import pandas as pd\n",
    "features = ['study', 'free', 'money', 'university']\n",
    "target = 'is_spam'\n",
    "messages = pd.DataFrame(\n",
    "  [(1, 0, 0, 0, 0),\n",
    "  (0, 0, 1, 1, 0),\n",
    "  (1, 0, 0, 0, 0),\n",
    "  (1, 1, 0, 0, 0)] +\n",
    "  [(0, 1, 0, 0, 1)] * 4 +\n",
    "  [(0, 1, 1, 0, 1)] * 4,\n",
    "columns=features+[target])\n",
    "messages\n",
    "\n",
    "# Fit a Naive Bayes classifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "X = messages[features]\n",
    "y = messages[target]\n",
    "cl = BernoulliNB().fit(X, y)\n",
    "\n",
    "# Predict the message \"study money university\"\n",
    "new_messages = pd.DataFrame(\n",
    "  [(1, 0, 1, 1)],\n",
    "columns = features)\n",
    "cl.predict_proba(new_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the prediction for \"study money university\" is about 1.98%, even if \"university\" never shows up in a spam message. This is because `BernoulliNB()` uses a technique called _additive smoothing_ or _Laplace smoothing_, controlled through the `alpha` parameter. It essentially adds a constant value (by default 1) to each variable count, as if each variable was seen once for every value of the target. This makes sure that no probability becomes exactly 0. The Python implementation does not allow us to completely disable the estimator, but we can see what happens if we set it to something very close to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 5.55545733e-32]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = BernoulliNB(alpha=1e-10).fit(X, y)\n",
    "cl.predict_proba(new_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By disabling the Laplace smoothing, the predictions are now 100% and 0% respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Bayes\n",
    "\n",
    "The independence assumption of naive Bayes could theoretically be dropped, creating an algorithm usually named _Joint Bayes_, which is simply looking for the value \n",
    "\n",
    "$$ v_{MAP} = \\mbox{argmax}_{v_j \\in V} P(a_1,a_2,...,a_n|v_j)P(v_j) $$\n",
    "\n",
    "Due to the exponential size of the model, this algorithm is not very practical, and consequently not available in the `scikit` library.\n",
    "\n",
    "Since the attributes are no longer considered independent, the conditional probability $P(a_1,a_2,...,a_n|v_j)$ can no longer be transformed to $\\prod_i P(a_i|v_j)P(v_j)$. If applied to a text dataset for instance, $n$ would be equal the number of words in a language and the model would have to learn all possible combinations of words in spam and non-spam, so $2 \\times 2^n = 2^{n+1}$. Since English has about $n=170,000$ words in current use, we can see how this approach quickly becomes unfeasable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
